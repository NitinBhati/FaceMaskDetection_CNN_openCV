{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "from os import getcwd\n",
    "from os import listdir\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import imutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image  as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of images with facemask labelled 'yes': 690\n",
      "The number of images with facemask labelled 'no': 686\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of images with facemask labelled 'yes':\",len(os.listdir('Dataset/yes')))\n",
    "print(\"The number of images with facemask labelled 'no':\",len(os.listdir('Dataset/no')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 2751\n",
      "Percentage of Yes examples: 50.163576881134134%, number of pos examples: 1380\n",
      "Percentage of No examples: 49.836423118865866%, number of neg examples: 1371\n"
     ]
    }
   ],
   "source": [
    "def data_summary(main_path):\n",
    "    \n",
    "    yes_path = main_path+'yesreal'\n",
    "    no_path = main_path+'noreal'\n",
    "        \n",
    "    # number of files (images) that are in the the folder named 'yes' \n",
    "    m_pos = len(listdir(yes_path))\n",
    "    # number of files (images) that are in the the folder named 'no' \n",
    "    m_neg = len(listdir(no_path))\n",
    "    # number of all examples\n",
    "    m = (m_pos+m_neg)\n",
    "    \n",
    "    pos_prec = (m_pos* 100.0)/ m\n",
    "    neg_prec = (m_neg* 100.0)/ m\n",
    "    \n",
    "    print(f\"Number of examples: {m}\")\n",
    "    print(f\"Percentage of yes examples: {pos_prec}%, number of pos examples: {m_pos}\") \n",
    "    print(f\"Percentage of no examples: {neg_prec}%, number of neg examples: {m_neg}\") \n",
    "    \n",
    "augmented_data_path = 'Dataset/trial1/augmented data1/'    \n",
    "data_summary(augmented_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
    "    dataset = []\n",
    "    \n",
    "    for unitData in os.listdir(SOURCE):\n",
    "        data = SOURCE + unitData\n",
    "        if(os.path.getsize(data) > 0):\n",
    "            dataset.append(unitData)\n",
    "        else:\n",
    "            print('Skipped ' + unitData)\n",
    "            print('Invalid file i.e zero size')\n",
    "    \n",
    "    train_set_length = int(len(dataset) * SPLIT_SIZE)\n",
    "    test_set_length = int(len(dataset) - train_set_length)\n",
    "    shuffled_set = random.sample(dataset, len(dataset))\n",
    "    train_set = dataset[0:train_set_length]\n",
    "    test_set = dataset[-test_set_length:]\n",
    "       \n",
    "    for unitData in train_set:\n",
    "        temp_train_set = SOURCE + unitData\n",
    "        final_train_set = TRAINING + unitData\n",
    "        copyfile(temp_train_set, final_train_set)\n",
    "    \n",
    "    for unitData in test_set:\n",
    "        temp_test_set = SOURCE + unitData\n",
    "        final_test_set = TESTING + unitData\n",
    "        copyfile(temp_test_set, final_test_set)\n",
    "        \n",
    "        \n",
    "YES_SOURCE_DIR = \"Dataset/trial1/augmented data1/yesreal/\"\n",
    "TRAINING_YES_DIR = \"Dataset/trial1/augmented data1/training/yes1/\"\n",
    "TESTING_YES_DIR = \"Dataset/trial1/augmented data1/testing/yes1/\"\n",
    "NO_SOURCE_DIR = \"Dataset/trial1/augmented data1/noreal/\"\n",
    "TRAINING_NO_DIR = \"Dataset/trial1/augmented data1/training/no1/\"\n",
    "TESTING_NO_DIR = \"Dataset/trial1/augmented data1/testing/no1/\"\n",
    "split_size = .8\n",
    "split_data(YES_SOURCE_DIR, TRAINING_YES_DIR, TESTING_YES_DIR, split_size)\n",
    "split_data(NO_SOURCE_DIR, TRAINING_NO_DIR, TESTING_NO_DIR, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of images with facemask in the training set labelled 'yes': 1104\n",
      "The number of images with facemask in the test set labelled 'yes': 276\n",
      "The number of images without facemask in the training set labelled 'no': 1096\n",
      "The number of images without facemask in the test set labelled 'no': 275\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of images with facemask in the training set labelled 'yes':\", len(os.listdir('Dataset/trial1/augmented data1/training/yes1')))\n",
    "print(\"The number of images with facemask in the test set labelled 'yes':\", len(os.listdir('Dataset/trial1/augmented data1/testing/yes1')))\n",
    "print(\"The number of images without facemask in the training set labelled 'no':\", len(os.listdir('Dataset/trial1/augmented data1/training/no1')))\n",
    "print(\"The number of images without facemask in the test set labelled 'no':\", len(os.listdir('Dataset/trial1/augmented data1/testing/no1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(100, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2200 images belonging to 2 classes.\n",
      "Found 551 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = \"Dataset/trial1/augmented data1/training\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
    "                                                    batch_size=10, \n",
    "                                                    target_size=(150, 150))\n",
    "VALIDATION_DIR = \"Dataset/trial1/augmented data1/testing\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
    "                                                         batch_size=10, \n",
    "                                                         target_size=(150, 150))\n",
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-fb50dbaa623d>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/30\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.5123 - acc: 0.7641WARNING:tensorflow:From C:\\Users\\bhati.n\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\bhati.n\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model-001.model\\assets\n",
      "220/220 [==============================] - 126s 574ms/step - loss: 0.5123 - acc: 0.7641 - val_loss: 0.2038 - val_acc: 0.9365\n",
      "Epoch 2/30\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.3481 - acc: 0.8618INFO:tensorflow:Assets written to: model-002.model\\assets\n",
      "220/220 [==============================] - 126s 573ms/step - loss: 0.3481 - acc: 0.8618 - val_loss: 0.2028 - val_acc: 0.9383\n",
      "Epoch 3/30\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.2509 - acc: 0.9109INFO:tensorflow:Assets written to: model-003.model\\assets\n",
      "220/220 [==============================] - 129s 585ms/step - loss: 0.2509 - acc: 0.9109 - val_loss: 0.1494 - val_acc: 0.9474\n",
      "Epoch 4/30\n",
      "220/220 [==============================] - 123s 560ms/step - loss: 0.2075 - acc: 0.9241 - val_loss: 0.1604 - val_acc: 0.9292\n",
      "Epoch 5/30\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.1681 - acc: 0.9327INFO:tensorflow:Assets written to: model-005.model\\assets\n",
      "220/220 [==============================] - 125s 567ms/step - loss: 0.1681 - acc: 0.9327 - val_loss: 0.1450 - val_acc: 0.9546\n",
      "Epoch 6/30\n",
      "220/220 [==============================] - 125s 568ms/step - loss: 0.1393 - acc: 0.9500 - val_loss: 0.1933 - val_acc: 0.9256\n",
      "Epoch 7/30\n",
      "220/220 [==============================] - 122s 554ms/step - loss: 0.1706 - acc: 0.9323 - val_loss: 0.2359 - val_acc: 0.8929\n",
      "Epoch 8/30\n",
      "220/220 [==============================] - 128s 584ms/step - loss: 0.1392 - acc: 0.9491 - val_loss: 0.1665 - val_acc: 0.9365\n",
      "Epoch 9/30\n",
      "220/220 [==============================] - 124s 564ms/step - loss: 0.1601 - acc: 0.9445 - val_loss: 0.1509 - val_acc: 0.9328\n",
      "Epoch 10/30\n",
      "220/220 [==============================] - 121s 549ms/step - loss: 0.1360 - acc: 0.9491 - val_loss: 0.1487 - val_acc: 0.9546\n",
      "Epoch 11/30\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.0939 - acc: 0.9682INFO:tensorflow:Assets written to: model-011.model\\assets\n",
      "220/220 [==============================] - 126s 572ms/step - loss: 0.0939 - acc: 0.9682 - val_loss: 0.1245 - val_acc: 0.9619\n",
      "Epoch 12/30\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.0846 - acc: 0.9718INFO:tensorflow:Assets written to: model-012.model\\assets\n",
      "220/220 [==============================] - 126s 574ms/step - loss: 0.0846 - acc: 0.9718 - val_loss: 0.1127 - val_acc: 0.9637\n",
      "Epoch 13/30\n",
      "220/220 [==============================] - 128s 580ms/step - loss: 0.1179 - acc: 0.9591 - val_loss: 0.1256 - val_acc: 0.9564\n",
      "Epoch 14/30\n",
      "220/220 [==============================] - 122s 556ms/step - loss: 0.1003 - acc: 0.9636 - val_loss: 0.1241 - val_acc: 0.9528\n",
      "Epoch 15/30\n",
      "220/220 [==============================] - 121s 550ms/step - loss: 0.0715 - acc: 0.9777 - val_loss: 0.1674 - val_acc: 0.9528\n",
      "Epoch 16/30\n",
      "220/220 [==============================] - 121s 550ms/step - loss: 0.0891 - acc: 0.9677 - val_loss: 0.1153 - val_acc: 0.9601\n",
      "Epoch 17/30\n",
      "220/220 [==============================] - 124s 564ms/step - loss: 0.0703 - acc: 0.9777 - val_loss: 0.1883 - val_acc: 0.9383\n",
      "Epoch 18/30\n",
      "220/220 [==============================] - 121s 552ms/step - loss: 0.0806 - acc: 0.9718 - val_loss: 0.1169 - val_acc: 0.9619\n",
      "Epoch 19/30\n",
      "220/220 [==============================] - 122s 557ms/step - loss: 0.1111 - acc: 0.9591 - val_loss: 0.1755 - val_acc: 0.9183\n",
      "Epoch 20/30\n",
      "220/220 [==============================] - 122s 556ms/step - loss: 0.1325 - acc: 0.9555 - val_loss: 0.1664 - val_acc: 0.9365\n",
      "Epoch 21/30\n",
      "220/220 [==============================] - 121s 551ms/step - loss: 0.0842 - acc: 0.9673 - val_loss: 0.1684 - val_acc: 0.9220\n",
      "Epoch 22/30\n",
      "220/220 [==============================] - 122s 555ms/step - loss: 0.0905 - acc: 0.9686 - val_loss: 0.1258 - val_acc: 0.9583\n",
      "Epoch 23/30\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.0887 - acc: 0.9677INFO:tensorflow:Assets written to: model-023.model\\assets\n",
      "220/220 [==============================] - 124s 562ms/step - loss: 0.0887 - acc: 0.9677 - val_loss: 0.1070 - val_acc: 0.9564\n",
      "Epoch 24/30\n",
      "220/220 [==============================] - 124s 564ms/step - loss: 0.0617 - acc: 0.9759 - val_loss: 0.1521 - val_acc: 0.9655\n",
      "Epoch 25/30\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.0646 - acc: 0.9786INFO:tensorflow:Assets written to: model-025.model\\assets\n",
      "220/220 [==============================] - 123s 561ms/step - loss: 0.0646 - acc: 0.9786 - val_loss: 0.1057 - val_acc: 0.9673\n",
      "Epoch 26/30\n",
      "220/220 [==============================] - ETA: 0s - loss: 0.0524 - acc: 0.9845INFO:tensorflow:Assets written to: model-026.model\\assets\n",
      "220/220 [==============================] - 126s 571ms/step - loss: 0.0524 - acc: 0.9845 - val_loss: 0.1032 - val_acc: 0.9637\n",
      "Epoch 27/30\n",
      "220/220 [==============================] - 120s 547ms/step - loss: 0.0532 - acc: 0.9832 - val_loss: 0.1116 - val_acc: 0.9637\n",
      "Epoch 28/30\n",
      "220/220 [==============================] - 120s 547ms/step - loss: 0.0841 - acc: 0.9745 - val_loss: 0.1185 - val_acc: 0.9528\n",
      "Epoch 29/30\n",
      "220/220 [==============================] - 129s 586ms/step - loss: 0.0647 - acc: 0.9773 - val_loss: 0.1051 - val_acc: 0.9601\n",
      "Epoch 30/30\n",
      "220/220 [==============================] - 123s 558ms/step - loss: 0.0487 - acc: 0.9855 - val_loss: 0.1257 - val_acc: 0.9601\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=30,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_clsfr=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-86a0855a7334>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mreshaped\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mreshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreshaped\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreshaped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;31m#print(result)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "labels_dict={0:'without_mask',1:'with_mask'}\n",
    "color_dict={0:(0,0,255),1:(0,255,0)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0) #Use camera 0\n",
    "\n",
    "# We load the xml file\n",
    "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im=cv2.flip(im,1,1) #Flip to act as a mirror\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "\n",
    "    # detect MultiScale / faces \n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "\n",
    "    # Draw rectangles around each face\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * size for v in f] #Scale the shapesize backup\n",
    "        #Save just the rectangle faces in SubRecFaces\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        resized=cv2.resize(face_img,(150,150))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,150,150,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    "        #print(result)\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "    # Show the image\n",
    "    cv2.imshow('LIVE',   im)\n",
    "    key = cv2.waitKey(10)\n",
    "    # if Esc key is press then break out of the loop \n",
    "    if key == 27: #The Esc key\n",
    "        break\n",
    "# Stop video\n",
    "webcam.release()\n",
    "\n",
    "# Close all started windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "new_model=load_model('model-026.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 100)     2800      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 100)       90100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 100)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 129600)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 129600)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                6480050   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 6,573,052\n",
      "Trainable params: 6,573,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict={0:'without_mask',1:'with_mask'}\n",
    "color_dict={0:(0,0,255),1:(0,255,0)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0) #Use camera 0\n",
    "\n",
    "# We load the xml file\n",
    "classifier = cv2.CascadeClassifier('haar_test.xml')\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im=cv2.flip(im,1,1) #Flip to act as a mirror\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "\n",
    "    # detect MultiScale / faces \n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "\n",
    "    # Draw rectangles around each face\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * size for v in f] #Scale the shapesize backup\n",
    "        #Save just the rectangle faces in SubRecFaces\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        resized=cv2.resize(face_img,(150,150))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,150,150,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=new_model.predict(reshaped)\n",
    "        #print(result)\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "    # Show the image\n",
    "    cv2.imshow('LIVE',   im)\n",
    "    key = cv2.waitKey(10)\n",
    "    # if Esc key is press then break out of the loop \n",
    "    if key == 27: #The Esc key\n",
    "        break\n",
    "# Stop video\n",
    "webcam.release()\n",
    "\n",
    "# Close all started windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict={0:'without_mask',1:'with_mask'}\n",
    "color_dict={0:(0,0,255),1:(0,255,0)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0) #Use camera 0\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 20.0, (640,480))\n",
    "\n",
    "# We load the xml file\n",
    "classifier = cv2.CascadeClassifier('haar_test.xml')\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im=cv2.flip(im,1,1) #Flip to act as a mirror\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "\n",
    "    # detect MultiScale / faces \n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "\n",
    "    # Draw rectangles around each face\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * size for v in f] #Scale the shapesize backup\n",
    "        #Save just the rectangle faces in SubRecFaces\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        resized=cv2.resize(face_img,(150,150))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,150,150,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=new_model.predict(reshaped)\n",
    "        #print(result)\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        out.write(im)    \n",
    "\n",
    "    # Show the image\n",
    "    cv2.imshow('LIVE',   im)\n",
    "    key = cv2.waitKey(10)\n",
    "    # if Esc key is press then break out of the loop \n",
    "    if key == 27: #The Esc key\n",
    "        break\n",
    "# Stop video\n",
    "webcam.release()\n",
    "out.release()\n",
    "# Close all started windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop video\n",
    "webcam.release()\n",
    "\n",
    "# Close all started windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
